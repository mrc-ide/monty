% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sampler-nested-adaptive.R
\name{monty_sampler_nested_adaptive}
\alias{monty_sampler_nested_adaptive}
\title{Nested Adaptive Metropolis-Hastings Sampler}
\usage{
monty_sampler_nested_adaptive(
  initial_vcv,
  initial_vcv_weight = 1000,
  initial_scaling = 1,
  initial_scaling_weight = NULL,
  min_scaling = 0,
  scaling_increment = NULL,
  log_scaling_update = TRUE,
  acceptance_target = 0.234,
  forget_rate = 0.2,
  forget_end = Inf,
  adapt_end = Inf,
  pre_diminish = 0,
  boundaries = "reflect"
)
}
\arguments{
\item{initial_vcv}{An initial variance covariance matrix; we'll start
using this in the proposal, which will gradually become more weighted
towards the empirical covariance matrix calculated from the chain.}

\item{initial_vcv_weight}{Weight of the initial variance-covariance
matrix used to build the proposal of the random-walk. Higher
values translate into higher confidence of the initial
variance-covariance matrix and means that update from additional
samples will be slower.}

\item{initial_scaling}{The initial scaling of the variance
covariance matrix to be used to generate the multivariate normal
proposal for the random-walk Metropolis-Hastings algorithm. To generate
the proposal matrix, the weighted variance covariance matrix is
multiplied by the scaling parameter squared times 2.38^2 / n_pars (where
n_pars is the number of fitted parameters). Thus, in a Gaussian target
parameter space, the optimal scaling will be around 1.}

\item{initial_scaling_weight}{The initial weight used in the scaling update.
The scaling weight will increase after the first \code{pre_diminish}
iterations, and as the scaling weight increases the adaptation of the
scaling diminishes. If \code{NULL} (the default) the value is
5 / (acceptance_target * (1 - acceptance_target)).}

\item{min_scaling}{The minimum scaling of the variance covariance
matrix to be used to generate the multivariate normal proposal
for the random-walk Metropolis-Hastings algorithm.}

\item{scaling_increment}{The scaling increment which is added or
subtracted to the scaling factor of the variance-covariance
after each adaptive step. If \code{NULL} (the default) then an optimal
value will be calculated.}

\item{log_scaling_update}{Logical, whether or not changes to the
scaling parameter are made on the log-scale.}

\item{acceptance_target}{The target for the fraction of proposals
that should be accepted (optimally) for the adaptive part of the
chain.}

\item{forget_rate}{The rate of forgetting early parameter sets from the
empirical variance-covariance matrix in the MCMC chains. For example,
\code{forget_rate = 0.2} (the default) means that once in every 5th iterations
we remove the earliest parameter set included, so would remove the 1st
parameter set on the 5th update, the 2nd on the 10th update, and so
on. Setting \code{forget_rate = 0} means early parameter sets are never
forgotten.}

\item{forget_end}{The final iteration at which early parameter sets can
be forgotten. Setting \code{forget_rate = Inf} (the default) means that the
forgetting mechanism continues throughout the chains. Forgetting early
parameter sets becomes less useful once the chains have settled into the
posterior mode, so this parameter might be set as an estimate of how long
that would take.}

\item{adapt_end}{The final iteration at which we can adapt the multivariate
normal proposal. Thereafter the empirical variance-covariance matrix, its
scaling and its weight remain fixed. This allows the adaptation to be
switched off at a certain point to help ensure convergence of the chain.}

\item{pre_diminish}{The number of updates before adaptation of the scaling
parameter starts to diminish. Setting \code{pre_diminish = 0} means there is
diminishing adaptation of the scaling parameter from the offset, while
\code{pre_diminish = Inf} would mean there is never diminishing adaptation.
Diminishing adaptation should help the scaling parameter to converge
better, but while the chains find the location and scale of the posterior
mode it might be useful to explore with it switched off.}

\item{boundaries}{Control the behaviour of proposals that are
outside the model domain.  The supported options are:
\itemize{
\item "reflect" (the default): we reflect proposed parameters that
lie outside the domain back into the domain (as many times as
needed)
\item "reject": we do not evaluate the density function, and return
\code{-Inf} for its density instead.
\item "ignore": evaluate the point anyway, even if it lies outside
the domain.
}

The initial point selected will lie within the domain, as this is
enforced by \link{monty_sample}.}
}
\value{
A \code{monty_sampler} object, which can be used with
\link{monty_sample}
}
\description{
Create a nested adaptive Metropolis-Hastings sampler, which extends the
adaptive sampler \link{monty_sampler_adaptive}, tuning the variance covariance
matrices for proposal for the separable sections
of a nested model (vs the simple nested random walk sampler
\link{monty_sampler_random_walk}). This sampler requires
that models support the \code{has_parameter_groups} property.
}
\details{
Much like the simple nested random walk sampler
\link{monty_sampler_random_walk}, the strategy is to propose all the
shared parameters as a deviation from the current point in parameter space
as a single move and accept or reject as a block. Then we generate points
for all the region-specific parameters, compute the density and then
accept or reject these updates independently.  This is possible
because the change in likelihood in region A is independent from
region B.

The adaptive proposal algorithm of the non-nested adaptive sampler
\link{monty_sampler_adaptive} is extended here to adaptively tune the variance
covariance matrix of each of these parameter chunks.
}
